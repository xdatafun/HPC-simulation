# HPC-simulation
Summary: This was part of my PhD dissertation. Item response data were simulated under various conditions, data were analyzed by Bayesian methods, multiple outputs were saved for later use, with major focus on posterior prediction distributionon. The R scripts were submitted as jobs on command line on high performance cluster (the HPC called Talon2 at University of North Texas).

## About The Project
### Item Response Theory and Scaled Scores
High-stake tests rarely use raw sum scores of correct answers as the final scores. For example, TOEFL used to have 677 as the full score, but there weren't 677 questions. Instead, 677 was a scaled score, meaning each test-taker answered 100-120 questions, each question had pre-calculated probability curve, as long as the integration of a set of questions met certain standard, raw scores could be transformed to a scaled score metric, and thus each test-taker would have a scaled score. The reason for this trouble is that since all questions in the same item bank have pre-calculated probability profiles, test-takers answering different questions can still have comparable scores. Without the pre-calculated item bank, scores are only comparable when test-takers use exactly the same set of questions. Item Response Theory (IRT) was invented in 60s to compute the probability curves for questions and nowadays most high-stake tests have adopted IRT.  

IRT is similar to confirmatory factor analysis (CFA) in that both are latent variable models, where questions (items) load on latent variables (representing attributes or abilities being measured). IRT uses item-level data, CFA usually starts with variance-covariance matrix among items (item-level CFA is getting popular in recent years, but I won't talk about it here).  

### Measurement Bias
One of the most essential questions that educational research and psychometric investigations are trying to answer is "is the test fair?". For instance, a standard reading test should not systematically be biased against certain minority group. One way of testing this is to compare probability curves computed from sub-samples. The subsample curves are rarely perfectly overlap, lots of ways to compare - key parameters, areas between curves, etc. Another common practice is to set up the same latent variable model for all sum-samples and set constraints that all sub-samples are equal. If the sub-sample model does not fit the data, at least one of the sub-sample has misfit, but locating the misfit could be cumbersome when there are a number of sub-samples. In both approaches, determining the cut-off metric is tricky at best, usually arbitray. Part of problem stems from the debate on statistical significance (for starters see work of Bruce Thompson, Jacob Cohen, Andrew Gelman, Jeff Gill on p-value), others come from non-pivotal model-fit indices in latent variable models. The problem impedes interpretation on how bad the misfit is among sub-samples, more importantly, what may help researchers make more informed decisions about their questionnaires/tests? 

Bayesian methods offer more direct way to assess model fit. We can see the distribution of the parameters and model fit indices, instead of theoretical distributions of them that constructed by standard errors. 

### Bayesian methods and HPC 
I like Bayesian methods because of its philosophy. The idea of Bayesian theorem is (deceivingly) simple -- "by updating our initial belief about something with objective new information, we get a new and improved belief" (["The Theory That Would Not Die" by Sharon Bertsch McGrayne](https://www.amazon.com/dp/B0050QB3EQ/ref=dp-kindle-redirect?_encoding=UTF8&btkr=1), a very interesting book on history of Bayes rule). But it was hardly as enjoyable to program it in R as to think about the ideas. Our lab desktops were merely one-year-old but still frequently crashed as I simulated data under various conditions, analyzed in Bayesian IRT, and summarized the results. R runs in RAM, it crashes when running out of memory. I separated conditions in less number per script but the problem stayed. 

So I turned to the high performance cluster. I was the first one from College of Education to run analysis on HPC. The nickname was Talon2, recently it's become [Talon3](https://hpc.unt.edu/). Eagle is the mascot of University of North Texas. The HPC team was very excited to expend their service to our field and even gave me a personal session. It didn't make much sense (Sorry guys, I was uninitiated about shell scripting and Unix 2 years ago). But after a few weeks, all my R scripts were migranted and tweaked. It NEVER melt down again! 

Without HPC, I probably would have spent 2-3 times on the simulation and guarding the lab computers like a nanny 24/7. 

### The Simulation
As oversimplified above, I wanted to use Bayesian methods on IRT models as a more direct way of assessing model fit and comparing parameters across sub-samples (groups). In short (go on with the oversimplification), I needed to create a variety of bias and see how it's captured in Bayesian IRT, especially in terms of posterior predictive distribution. Aggregating biased sub-samples together, the mega sample was estimated and its posterior predictive distribution was created, then each subsample was estimated so that the sub-sample parameters can be marked in the mega sample posterior predictive distribution. Model fit indices were compared different, because some were no longer comparable once the sample size changed. 

Creating posterior predictive distribution (PPD) needs great computational power because each set of values from the posterior distribution is used to generate a hypothetical replication of the original data, each replication is estimated, then the all the results based on replications are pooled to construct the PPD. For instance, 1000 samples drawn from the converged chain are used to construct posterior distribution, then 1000 replications are created, each being estimated, 1000 results are pooled to form PPD. If one Bayesian IRT analysis cost 6 minutes, 1001 analyses will cost 100.1 hours. Each condition needs to be repeated 100 times, it will take 10,010 hours for each condition. I got a few hundreds of conditions. HPC offered parallel computing, I usually called 16 cores (I was modest and considerate, with Talon3 it could be a lot more). 

The conclusion is PPD can capture the bias at least as good as other common practices, offering the advantage of visualize the item differential functioning and misfit. 

The sample script 'sim_group500.R' yielded several outputs, including the matrix with results from replications (to create PPD) as well as intermidiate results that may be used for a closer look (so I didn't have to re-run everything). 

[to be continued more R scripts]
